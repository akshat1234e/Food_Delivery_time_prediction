# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GvQyi3hBokps_6UF4Wr3O1fu--A0sHjh

### Food Delivery Time Prediction Project

This notebook aims to predict food delivery times using various factors and evaluate the performance of different models.

###Data Loading and Initial Exploration

This cell loads the dataset, displays its shape, column names, the first few rows, checks data types and non-null counts, and shows missing values.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv('Food_Delivery_Time_Prediction.csv')


print("Shape of the dataset:", df.shape)
print("\nColumn names:\n", df.columns.tolist())

print(df.head())


print("\nDataset info:")
print(df.info())


print("\nMissing values per column:")
print(df.isnull().sum())

"""### Statistical Summary and Visualization

This cell provides a statistical summary of numerical features, plots histograms to show their distributions, calculates the correlation matrix, and visualizes correlations with 'Delivery\_Time'.
"""

print("Basic Statistical Summary:\n")
print(df.describe().T)

df.hist(figsize=(12, 10), bins=15, edgecolor='black')
plt.suptitle("Distribution of Numerical Features", fontsize=16)
plt.show()


correlation = df.select_dtypes(include=['float64', 'int64']).corr()


print("Correlation with Delivery_Time:\n")
print(correlation['Delivery_Time'].sort_values(ascending=False))

correlation['Delivery_Time'].sort_values(ascending=False).plot(kind='bar', color='skyblue')
plt.title("Correlation of Features with Delivery_Time")
plt.ylabel("Correlation Coefficient")
plt.show()

"""###Outlier Handling

This cell identifies outliers in specific numerical columns ('Order\_Cost', 'Tip\_Amount', 'Customer\_Rating', 'Distance') using the IQR method and caps these outlier values to the calculated bounds.
"""

# Identify numerical columns for outlier detection
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

# Exclude the target variable and encoded categorical variables if they are now numeric

outlier_cols = ['Order_Cost','Tip_Amount','Customer_Rating','Distance']


print("Columns for outlier detection:", outlier_cols)

# Apply IQR-based outlier handling using only pandas operations
for col in outlier_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap the outliers using pandas .loc
    df.loc[df[col] < lower_bound, col] = lower_bound
    df.loc[df[col] > upper_bound, col] = upper_bound

print("\nOutliers handled by capping for selected numerical columns using pandas.")

"""###Feature Engineering: Haversine Function and Distance Calculation

This cell defines a function to calculate the geographic distance between two points using the Haversine formula, extracts latitude and longitude from location strings, and applies the Haversine function to calculate and store the distance between customer and restaurant locations.

**Reasoning**:
Define a function to calculate the distance between two points using the Haversine formula.
"""

def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(np.radians, [lat1, lon1, lat2, lon2])

    dlon = lon2_rad - lon1_rad
    dlat = lat2_rad - lat1_rad

    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    c = 2 * np.arcsin(np.sqrt(a))

    distance = R * c
    return distance

def extract_lat_lon(location_str):
    """Extracts latitude and longitude from a string like '(lat, lon)'."""
    try:
        # Remove parentheses and split by comma
        lat_lon = location_str.replace('(', '').replace(')', '').split(',')
        # Convert to float and return
        return float(lat_lon[0]), float(lat_lon[1])
    except:
        return None, None # Handle potential errors

df['Customer_latitude'], df['Customer_longitude'] = zip(*df['Customer_Location'].apply(extract_lat_lon))
df['Restaurant_latitude'], df['Restaurant_longitude'] = zip(*df['Restaurant_Location'].apply(extract_lat_lon))


df['Distance'] = df.apply(lambda row: haversine(row['Restaurant_latitude'], row['Restaurant_longitude'],
                                                row['Customer_latitude'], row['Customer_longitude']), axis=1)

print("DataFrame with Distance column:")
display(df.head())

"""###Gaussian Naive Bayes Model

This cell prepares the data by dropping irrelevant columns and creating a binary 'Delivery\_Status' target variable, splits the data, encodes categorical features using LabelEncoder, trains a Gaussian Naive Bayes classifier, makes predictions, and evaluates the model using accuracy, classification report, confusion matrix, and a Precision-Recall curve.
"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, PrecisionRecallDisplay, RocCurveDisplay
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# --- Consolidated Data Preparation ---
# Create the binary target variable 'Delivery_Status' based on average Delivery_Time
avg_time = df['Delivery_Time'].mean()
df['Delivery_Status'] = np.where(df['Delivery_Time'] <= avg_time, 1, 0)

# Define features (X_features) and target (Y_target)
X_features = df.drop(['Delivery_Time', 'Order_ID', 'Customer_Location', 'Restaurant_Location', 'Restaurant_latitude', 'Restaurant_longitude', 'Delivery_Status'], axis=1)
Y_target = df['Delivery_Status']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_features, Y_target, test_size=0.2, random_state=42)

# Identify categorical and numerical columns in the training data
categorical_cols = X_train.select_dtypes(include=['object']).columns
numerical_cols = X_train.select_dtypes(include=np.number).columns

# Apply LabelEncoder to categorical columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])
    label_encoders[col] = le  # Store the encoder

# Apply Standardization to numerical columns
scaler = StandardScaler()
X_train_processed = X_train.copy()
X_test_processed = X_test.copy()
X_train_processed[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test_processed[numerical_cols] = scaler.transform(X_test[numerical_cols])

print("Data preparation complete: target variable created, data split, categorical features encoded, and numerical features scaled.")
print(f"Shape of X_train_processed: {X_train_processed.shape}")
print(f"Shape of X_test_processed: {X_test_processed.shape}")
# --- End Consolidated Data Preparation ---


# Gaussian Naive Bayes model
model = GaussianNB()

# Cross-validation (Optional, but good practice)
cv_scores = cross_val_score(model, X_train_processed, y_train, cv=5) # Use processed data
print(f"\nCross-validation Accuracy Scores: {cv_scores}")
print(f"Mean Cross-validation Accuracy: {cv_scores.mean():.2f}")

model.fit(X_train_processed, y_train) # Use processed data

y_pred = model.predict(X_test_processed) # Use processed data

accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy on Test Set: {accuracy:.2f}")

print("\nClassification Report:\n", classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", cm)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix - Gaussian Naive Bayes')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot Precision-Recall Curve
precision_recall_display = PrecisionRecallDisplay.from_estimator(model, X_test_processed, y_test, name="Gaussian Naive Bayes") # Use processed data
_ = precision_recall_display.ax_.set_title("2-class Precision-Recall curve - Gaussian Naive Bayes")
plt.show()

# For ROC Curve
plt.figure(figsize=(8, 6))
RocCurveDisplay.from_estimator(model, X_test_processed, y_test) # Use processed data
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve - Gaussian Naive Bayes')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""###K-Nearest Neighbors Model

This cell prepares the data, splits it into training and testing sets, encodes categorical features using LabelEncoder, trains a K-Nearest Neighbors classifier (with n=5), makes predictions, and evaluates the model using accuracy, classification report, confusion matrix, and a Precision-Recall curve.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, PrecisionRecallDisplay, RocCurveDisplay
import matplotlib.pyplot as plt
import seaborn as sns

# K-Nearest Neighbors model
knn = KNeighborsClassifier()

# Hyperparameter Tuning with GridSearchCV
param_grid = {'n_neighbors': np.arange(1, 21)} # Test n_neighbors from 1 to 20
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy') # 5-fold cross-validation
grid_search.fit(X_train_processed, y_train) # Use processed data from previous cell

best_k = grid_search.best_params_['n_neighbors']
print(f"Best k for KNN: {best_k}")

# Train KNN model with the best k
knn_best = KNeighborsClassifier(n_neighbors=best_k)

# Cross-validation with the best model (Optional)
cv_scores = cross_val_score(knn_best, X_train_processed, y_train, cv=5) # Use processed data
print(f"\nCross-validation Accuracy Scores (Best KNN): {cv_scores}")
print(f"Mean Cross-validation Accuracy (Best KNN): {cv_scores.mean():.2f}")

knn_best.fit(X_train_processed, y_train) # Use processed data

y_pred = knn_best.predict(X_test_processed) # Use processed data

accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy on Test Set (Best KNN): {accuracy:.2f}")

print("\nClassification Report (Best KNN):\n", classification_report(y_test, y_pred))

cnm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix (Best KNN):\n", cnm)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cnm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix - KNN')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot Precision-Recall Curve
precision_recall_display = PrecisionRecallDisplay.from_estimator(knn_best, X_test_processed, y_test, name="KNeighborsClassifier (Best k)") # Use processed data
_ = precision_recall_display.ax_.set_title("2-class Precision-Recall curve - KNN")
plt.show()

# For ROC Curve
plt.figure(figsize=(8, 6))
RocCurveDisplay.from_estimator(knn_best, X_test_processed, y_test) # Use processed data
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve - KNN')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""###Decision Tree Model

This cell prepares and splits the data, encodes categorical features, trains a Decision Tree classifier, makes predictions, and evaluates the model using accuracy, classification report, confusion matrix (visualized with a heatmap), a Precision-Recall curve, and an ROC curve.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, PrecisionRecallDisplay, roc_curve, auc, RocCurveDisplay
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Decision Tree model
dt = DecisionTreeClassifier(random_state=50)

# Hyperparameter Tuning with GridSearchCV
param_grid = {'max_depth': np.arange(1, 21), # Test max_depth from 1 to 20
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4]}
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy') # 5-fold cross-validation
grid_search.fit(X_train_processed, y_train) # Use processed data from previous cell

best_params = grid_search.best_params_
print(f"Best parameters for Decision Tree: {best_params}")

# Train Decision Tree model with the best parameters
dt_best = DecisionTreeClassifier(random_state=50, **best_params)

# Cross-validation with the best model (Optional)
cv_scores = cross_val_score(dt_best, X_train_processed, y_train, cv=5) # Use processed data
print(f"\nCross-validation Accuracy Scores (Best Decision Tree): {cv_scores}")
print(f"Mean Cross-validation Accuracy (Best Decision Tree): {cv_scores.mean():.2f}")

dt_best.fit(X_train_processed, y_train) # Use processed data

y_pred = dt_best.predict(X_test_processed) # Use processed data

accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy on Test Set (Best Decision Tree): {accuracy:.2f}")

print("\nClassification Report (Best Decision Tree):\n", classification_report(y_test, y_pred))

con = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix (Best Decision Tree):\n", con)

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(con, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix - Decision Tree')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()


# Plot Precision-Recall Curve
precision_recall_display = PrecisionRecallDisplay.from_estimator(dt_best, X_test_processed, y_test, name="DecisionTreeClassifier (Best Params)") # Use processed data
_ = precision_recall_display.ax_.set_title("2-class Precision-Recall curve - Decision Tree")
plt.show()

# Plot ROC Curve
# Get probability estimates for the positive class
y_prob = dt_best.predict_proba(X_test_processed)[:, 1] # Use processed data
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
RocCurveDisplay.from_estimator(dt_best, X_test_processed, y_test) # Use processed data
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC) Curve - Decision Tree')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show()

"""### Distance Distribution by Delivery Status

This visualization shows the distribution of the 'Distance' feature for each category of the 'Delivery\_Status' (Fast or Slow). This can help understand how distance influences whether a delivery is classified as fast or slow.
"""

plt.figure(figsize=(8, 6))
sns.boxplot(x='Delivery_Status', y='Distance', data=df) # Using boxplot to show distribution summary
plt.title('Distribution of Distance by Delivery Status')
plt.xlabel('Delivery Status (0: Slow, 1: Fast)')
plt.ylabel('Distance (km)')
plt.xticks(ticks=[0, 1], labels=['Slow (0)', 'Fast (1)']) # Add labels for clarity
plt.show()

"""### ROC Curve Comparison for All Models

This plot displays the Receiver Operating Characteristic (ROC) curves for the Gaussian Naive Bayes, K-Nearest Neighbors, and Decision Tree models on a single graph. This allows for a direct visual comparison of their trade-off between the True Positive Rate (Sensitivity) and the False Positive Rate (1-Specificity) across different classification thresholds.
"""

from sklearn.metrics import RocCurveDisplay
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
ax = plt.gca()

# Plot ROC for Gaussian Naive Bayes
RocCurveDisplay.from_estimator(model, X_test_processed, y_test, ax=ax, name="Gaussian Naive Bayes")

# Plot ROC for K-Nearest Neighbors
RocCurveDisplay.from_estimator(knn_best, X_test_processed, y_test, ax=ax, name="K-Nearest Neighbors")

# Plot ROC for Decision Tree
RocCurveDisplay.from_estimator(dt_best, X_test_processed, y_test, ax=ax, name="Decision Tree")

plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random Classifier')
plt.title('Receiver Operating Characteristic (ROC) Curve - All Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""###Apriori Algorithm:
to select relevant categorical columns from the  DataFrame and preprocess them into a one-hot encoded transactional format suitable for the Apriori algorithm, as outlined in the subtask instructions.


"""

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.preprocessing import OneHotEncoder

df_apriori = df.copy()

categorical_cols_for_apriori = ['Weather_Conditions', 'Traffic_Conditions', 'Order_Priority', 'Order_Time', 'Vehicle_Type', 'Delivery_Status']

# Convert 'Delivery_Status' to string for consistent one-hot encoding if it's not already, and to differentiate it
df_apriori['Delivery_Status'] = df_apriori['Delivery_Status'].astype(str)

# Apply One-Hot Encoding
# The drop_first=False is important here to retain all categories for association rules
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
ohe_data = ohe.fit_transform(df_apriori[categorical_cols_for_apriori])

# Create a DataFrame from the one-hot encoded data
transactions_df = pd.DataFrame(ohe_data, columns=ohe.get_feature_names_out(categorical_cols_for_apriori))

print("Shape of transactional DataFrame:", transactions_df.shape)
print("First 5 rows of transactional DataFrame:")
print(transactions_df.head())

# Generate frequent itemsets
frequent_itemsets = apriori(transactions_df, min_support=0.1, use_colnames=True)

print(frequent_itemsets.head())
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)

print("Association Rules found (min_confidence=0.7):")
print(rules.head())

"""### Apriori Algorithm Analysis

The Apriori algorithm was applied to the one-hot encoded categorical features, including the `Delivery_Status` (converted to string) to find frequent itemsets and association rules.

**Frequent Itemsets:**

- The frequent itemsets were generated with a `min_support` of 0.1. This means that only combinations of items appearing in at least 10% of the transactions were considered frequent. The head of the frequent itemsets DataFrame shows individual conditions like 'Weather_Conditions_Cloudy', 'Weather_Conditions_Rainy', etc., as frequent, along with some combinations. This suggests that these individual conditions are prevalent in the dataset.

**Association Rules:**

- The association rules were generated with a `min_confidence` of 0.7. However, the output for the association rules is an `Empty DataFrame`. This indicates that no rules were found where the confidence (the probability of `consequent` given `antecedent`) was 70% or higher, given the frequent itemsets generated.

## Food Delivery Time Prediction Project Report

**Project Goal:** To analyze factors influencing food delivery time and build classification models to predict if a delivery will be "fast" or "slow."

**Data Used:** Food Delivery Time Prediction dataset containing information on orders, locations, weather, traffic, ratings, and delivery times.

**Key Steps Performed:**

1.  **Data Loading & Exploration:** Loaded the dataset, examined its structure, identified data types, and checked for missing values (none found).
2.  **Statistical Summary & Visualization:** Generated descriptive statistics for numerical features and visualized their distributions using histograms. Analyzed correlations between numerical features and 'Delivery\_Time'.
3.  **Outlier Handling:** Identified and capped outliers in selected numerical columns ('Order\_Cost', 'Tip\_Amount', 'Customer\_Rating', 'Distance') using the IQR method.
4.  **Feature Engineering (Distance Calculation):** Defined and applied the Haversine formula to calculate the geographic distance between customer and restaurant locations based on latitude and longitude extracted from location strings.
5.  **Data Preparation for Modeling:** Created a binary target variable 'Delivery\_Status' (fast/slow) based on the average delivery time. Dropped irrelevant columns and encoded categorical features into numerical representations using Label Encoding. Split the data into training and testing sets.
6. ## Model Performance Comparison

Here's a comparison of the performance metrics for the three classification models trained after incorporating hyperparameter tuning, cross-validation, and feature scaling. The metrics are based on their performance on the test set and cross-validation scores.

| Model             | Mean CV Accuracy | Test Accuracy | Precision (Class 0) | Recall (Class 0) | F1-score (Class 0) | Precision (Class 1) | Recall (Class 1) | F1-score (Class 1) |
|-------------------|-------------------|---------------|---------------------|------------------|--------------------|---------------------|------------------|--------------------|
| Naive Bayes       | 0.47              | 0.53          | 0.57                | 0.55             | 0.56               | 0.47                | 0.50             | 0.49               |
| KNN (Best k=1)    | 0.50              | 0.55          | 0.60                | 0.55             | 0.57               | 0.50                | 0.56             | 0.53               |
| Decision Tree (Best Params) | 0.51              | 0.60          | 0.61                | 0.87             | 0.71               | 0.57                | 0.24             | 0.33               |

**Observations:**

*   After fixing the target leakage, the **K-Nearest Neighbors (KNN)** model's performance on the test set is 0.55 with a mean cross-validation accuracy of 0.50. The best `k` value was found to be 1. The ROC curve for KNN shows it performs slightly better than random chance.
*   The **Gaussian Naive Bayes** model, after fixing target leakage, showed a test accuracy of 0.53 and a mean cross-validation accuracy of 0.47. Its ROC curve also indicates performance marginally better than a random classifier.
*   The **Decision Tree** model, which already handled target leakage, achieved a test accuracy of 0.60 and a mean cross-validation accuracy of 0.51. The ROC curve for the Decision Tree shows an AUC of 0.55, which is close to random, especially for Class 1.

7.  **Apriori Algorithm:** Applied Apriori to one-hot encoded categorical features, including `Delivery_Status`, to find frequent itemsets and association rules.


**Summary of Findings:**

*   After correcting for target leakage, all three classification models (Gaussian Naive Bayes, KNN, Decision Tree) demonstrated modest predictive performance on the test set, with accuracies ranging from 0.53 to 0.60. Their mean cross-validation accuracies were also similar, indicating that the models are generalizing somewhat, but not strongly predicting fast/slow delivery times.
*   The ROC curves for all models showed AUC values close to 0.5, suggesting their discriminative ability is limited and they perform only slightly better than random guessing.
*   The Apriori algorithm did not yield any strong association rules with a minimum confidence of 0.7, implying that simple, highly confident rules are not easily discoverable among the selected categorical features for predicting delivery status. This suggests that the interactions between features might be more complex or that the chosen features do not have very strong, direct associations with delivery speed.

**Actionable Conclusions & Model Suitability:**

Given the current performance, none of the models stand out as a strong predictor for food delivery time being "fast" or "slow."

*   **Decision Tree:** Although it achieved the highest test accuracy (0.60), its low recall for Class 1 (fast delivery) means it struggles to correctly identify actual fast deliveries. This might be acceptable if the cost of misclassifying a slow delivery as fast is low, but not if identifying fast deliveries is crucial. Its simplicity and interpretability could be an advantage, but its predictive power is weak.
*   **K-Nearest Neighbors (KNN):** With a test accuracy of 0.55, its performance is similar to a random guess. The low value of best `k=1` found by `GridSearchCV` often suggests that the model is highly sensitive to individual data points and potentially overfitting on the training data, even with cross-validation. The corrected performance indicates it's not a reliable model for this problem.
*   **Gaussian Naive Bayes:** Its performance is on par with KNN. Naive Bayes assumes independence of features, which is rarely true in real-world data and might be contributing to its limited accuracy.

**Overall Recommendation:**
Based on the current analysis, there isn't a clear "best" model among the three for robustly predicting food delivery time as "fast" or "slow" in a binary sense. All models exhibit performance close to random, implying that the current features, preprocessing steps, or the binary classification approach might not be sufficient to capture the underlying patterns effectively.

**Trade-offs:**
*   **Simplicity vs. Performance:** While the models are relatively simple, their performance is not compelling. More complex models might be needed, but they would come at the cost of interpretability.
*   **Predictive Power vs. Data:** The current dataset might lack the granularity or specific features required for accurate binary delivery time prediction.

**Recommendations for Future Work:**

*   **Feature Engineering Enhancement:** Explore more sophisticated feature engineering. For example, derive features from `Order_Time` (e.g., hour of day, day of week), combine `Weather_Conditions` and `Traffic_Conditions` into an "overall condition" score, or consider polynomial features for numerical variables.
*   **Advanced Models:** Experiment with more powerful classification algorithms like Random Forests, Gradient Boosting Machines (e.g., XGBoost, LightGBM), or even simple neural networks.
*   **Collect More Data:** If feasible, acquiring a larger dataset with more diverse features (e.g., restaurant preparation time, delivery person's specific route, real-time traffic data, specific food items ordered) could significantly improve model performance.
*   **Class Imbalance:** Investigate if there's any class imbalance in `Delivery_Status` and address it if necessary, as it can affect model training and evaluation metrics.
*   **Domain Expertise:** Consult with domain experts to identify crucial factors influencing delivery times that might not be present in the current dataset or are not adequately captured by the existing features.

This comprehensive report, including the re-evaluated model performances and Apriori insights, provides a clear understanding of the project's current state and highlights actionable paths for future improvements.

### Insights or Next Steps
*   Given the weak predictive power of the current classification models, it is recommended to re-evaluate the problem as a regression task to predict the actual `Delivery_Time`, which might provide more granular insights.
*   Future work should focus on advanced feature engineering (e.g., temporal features, combined weather/traffic scores) and exploring more sophisticated machine learning algorithms (e.g., Random Forests, Gradient Boosting Machines) that might better capture complex patterns.
"""